{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TUfAcER1oUS6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb7qyhNL1yWt"
   },
   "source": [
    "# Train a detector with TensorFlow Lite Model Maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcLF2PKkSbV3"
   },
   "source": [
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vvAObmTqglq"
   },
   "source": [
    "### Install the required packages\n",
    "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhl8lqVamEty"
   },
   "outputs": [],
   "source": [
    "#!pip install -q tflite-model-maker\n",
    "#!pip install -q pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6lRhVK9Q_0U"
   },
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtxiUeZEiXpt",
    "outputId": "6b1871c7-5685-46a5-e855-d631e618d26c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/anaconda3/envs/marco-env/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.4.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tflite_model_maker\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRd13bfetO7B"
   },
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5M8iuydhVae"
   },
   "source": [
    "## Train your salad detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xushUyZXqP59"
   },
   "source": [
    "There are six steps to training an object detection model:\n",
    "\n",
    "**Step 1. Choose an object detection model archiecture.**\n",
    "\n",
    "This tutorial uses the EfficientDet-Lite2 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture. \n",
    "\n",
    "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
    "\n",
    "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
    "|--------------------|-----------|---------------|----------------------|\n",
    "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
    "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
    "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
    "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
    "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
    "\n",
    "<i> * Size of the integer quantized models. <br/>\n",
    "** Latency measured on Pixel 4 using 4 threads on CPU. <br/>\n",
    "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
    "</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtdZ-JDwMimd",
    "outputId": "14ebcba7-2cbe-4a27-b53f-4e7add096cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#spec1 = model_spec.get('efficientdet_lite2')\n",
    "spec2 = tflite_model_maker.object_detector.EfficientDetLite2Spec(\n",
    "    model_name='efficientdet-lite2',\n",
    "    uri='https://tfhub.dev/tensorflow/efficientdet/lite2/feature-vector/1', \n",
    "    hparams='', model_dir=None, epochs=10, batch_size=64,\n",
    "    steps_per_execution=1, moving_average_decay=0,\n",
    "    var_freeze_expr='(efficientnet|fpn_cells|resample_p6)',\n",
    "    tflite_max_detections=25, strategy=None, tpu=None, gcp_project=None,\n",
    "    tpu_zone=None, use_xla=False, profile=False, debug=False, tf_random_seed=111111,\n",
    "    verbose=0)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5U-A3tw6Y27"
   },
   "source": [
    "**Step 2. Load the dataset.**\n",
    "\n",
    "Model Maker will take input data in the CSV format. Use the `ObjectDetectorDataloader.from_csv` method to load the dataset and split them into the training, validation and test images.\n",
    "\n",
    "* Training images: These images are used to train the object detection model to recognize salad ingredients.\n",
    "* Validation images: These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "* Test images: These images are used to evaluate the final model performance.\n",
    "\n",
    "You can load the CSV file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. You can specify a local CSV file on your computer, and Model Maker will work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXU6jav5r1cW",
    "outputId": "9b7a236e-0666-424b-b457-91258527709c"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/marcoruizrueda/Carla-Object-Detection-Dataset.git carla_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV from xmls (FUNCIONA ok!)\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# source and credits:\n",
    "# https://raw.githubusercontent.com/datitran/raccoon_dataset/master/xml_to_csv.py\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text.replace(\".png\", \".jpeg\"),\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "\n",
    "def train():\n",
    "    image_path = os.path.join('train/annotations_dir')\n",
    "    xml_df = xml_to_csv(image_path)\n",
    "    labels_path = os.path.join('train/annotations_dir/train.csv')\n",
    "    xml_df.to_csv(labels_path, index=None)\n",
    "    print('> tf_wider_train - Successfully converted xml to csv.')\n",
    "\n",
    "def val():\n",
    "    image_path = os.path.join('test/annotations_dir/')\n",
    "    xml_df = xml_to_csv(image_path)\n",
    "    labels_path = os.path.join('test/annotations_dir/val.csv')\n",
    "    xml_df.to_csv(labels_path, index=None)\n",
    "    print('> tf_wider_val -  Successfully converted xml to csv.')\n",
    "\n",
    "train()\n",
    "val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReGGmfUtwLZu",
    "outputId": "1dc47daf-7c90-4c03-8e16-b749bf0d483d"
   },
   "outputs": [],
   "source": [
    "# Create TFRECORD from csv\n",
    "# Change train or test by hand\n",
    "#!python carla_training/generate_tfrecord.py --csv_input='carla_training/train/annotations_dir/train.csv' --output_path='carla_training/train/annotations_dir/train.record' --image_dir='carla_training/train/images_dir/'\n",
    "#!python carla_training/generate_tfrecord.py --csv_input='carla_training/test/annotations_dir/val.csv' --output_path='carla_training/test/annotations_dir/val.record' --image_dir='carla_training/test/images_dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HD5BvzWe6YKa",
    "outputId": "5e0737e6-003b-485b-9a0c-cba9d24dc41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#train_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')\n",
    "\n",
    "#label_map={1:'Vehicle', 2: 'Bike', 3: 'Motobike', 4: 'Traffic light', 5: 'Traffic sign'},\n",
    "\n",
    "#train_data, validation_data, test_data = object_detector.DataLoader.from_pascal_voc(\n",
    "#    images_dir='carla_training/Carla-Object-Detection-Dataset/train_data/images_dir/',\n",
    "#    annotations_dir='carla_training/Carla-Object-Detection-Dataset/train_data/annotations_dir/',\n",
    "#    label_map=['Vehicle', 'Bike', 'Motobike', 'Traffic light', 'Traffic sign'],\n",
    "#)\n",
    "\n",
    "train_data = object_detector.DataLoader(\n",
    "    '/home/marco/carla_training/train/annotations_dir/train.record', \n",
    "    size=820, \n",
    "    label_map={1:'vehicle', 2: 'bike', 3: 'motobike', 4: 'traffic_light', 5: 'traffic_sign'}, \n",
    "    annotations_json_file=None\n",
    ")\n",
    "validation_data = object_detector.DataLoader(\n",
    "    '/home/marco/carla_training/test/annotations_dir/val.record', \n",
    "    size=208, \n",
    "    label_map={1:'vehicle', 2: 'bike', 3: 'motobike', 4: 'traffic_light', 5: 'traffic_sign'}, \n",
    "    annotations_json_file=None\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mkru6NjjV70Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip uninstall numpy -y\n",
    "#!pip install update numpy==1.17.4\n",
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uZkLR6N6gDR"
   },
   "source": [
    "**Step 3. Train the TensorFlow model with the training data.**\n",
    "\n",
    "* The EfficientDet-Lite0 model uses `epochs = 50` by default, which means it will go through the training dataset 50 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
    "* Set `batch_size = 8` here so you will see that it takes 21 steps to go through the 175 images in the training dataset. \n",
    "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tfDZNtL2IVP",
    "outputId": "c9e04156-c16b-48bd-e723-5cb56bb93dc6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kwlYdTcg63xy",
    "outputId": "d9c2d8eb-1094-4def-f154-f360d79fe639"
   },
   "outputs": [],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec2, batch_size=8, train_whole_model=True, validation_data=validation_data, do_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BzCHLWJ6h7q"
   },
   "source": [
    "**Step 4. Evaluate the model with the test data.**\n",
    "\n",
    "After training the object detection model using the images in the training dataset, use the remaining 25 images in the test dataset to evaluate how the model performs against new data it has never seen before.\n",
    "\n",
    "As the default batch size is 64, it will take 1 step to go through the 25 images in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xmnl6Yy7ARn",
    "outputId": "6efd63db-622a-42b6-f373-41ef1756e6e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 [=====================>........] - ETA: 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.3293328,\n",
       " 'AP50': 0.49222553,\n",
       " 'AP75': 0.3835527,\n",
       " 'APs': 0.16599874,\n",
       " 'APm': 0.6696494,\n",
       " 'APl': 0.8652824,\n",
       " 'ARmax1': 0.021558926,\n",
       " 'ARmax10': 0.14325427,\n",
       " 'ARmax100': 0.37282896,\n",
       " 'ARs': 0.22770302,\n",
       " 'ARm': 0.70047987,\n",
       " 'ARl': 0.87666667,\n",
       " 'AP_/vehicle': 0.3275013,\n",
       " 'AP_/bike': 0.52024144,\n",
       " 'AP_/motobike': 0.6557059,\n",
       " 'AP_/traffic_light': 0.0679929,\n",
       " 'AP_/traffic_sign': 0.07522238}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgCDMe0e6jlT"
   },
   "source": [
    "**Step 5.  Export as a TensorFlow Lite model.**\n",
    "\n",
    "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Hm_UULdW7A9T"
   },
   "outputs": [],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UygYErfCD5m3"
   },
   "source": [
    "**Step 6.  Evaluate the TensorFlow Lite model.**\n",
    "\n",
    "Several factors can affect the model accuracy when exporting to TFLite:\n",
    "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop. \n",
    "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
    "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
    "\n",
    "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHYDWcljr6jq",
    "outputId": "9286a161-41d7-4097-9d7a-6f604b93793c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/208 [========================>.....] - ETA: 2:24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.32025373,\n",
       " 'AP50': 0.48261288,\n",
       " 'AP75': 0.38005114,\n",
       " 'APs': 0.16053818,\n",
       " 'APm': 0.6556571,\n",
       " 'APl': 0.8729645,\n",
       " 'ARmax1': 0.023187462,\n",
       " 'ARmax10': 0.14158987,\n",
       " 'ARmax100': 0.35480383,\n",
       " 'ARs': 0.22044155,\n",
       " 'ARm': 0.67954314,\n",
       " 'ARl': 0.8829167,\n",
       " 'AP_/vehicle': 0.32600158,\n",
       " 'AP_/bike': 0.51501876,\n",
       " 'AP_/motobike': 0.616035,\n",
       " 'AP_/traffic_light': 0.06708762,\n",
       " 'AP_/traffic_sign': 0.077125706}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_tflite('model.tflite', validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVxaf3x_7OfB"
   },
   "source": [
    "In the next step of the codelab, you'll use the [ObjectDetector API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) of the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview) to integrate the model into the Android app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1egWlephdND"
   },
   "source": [
    "## (Optional) Test the TFLite model on your image\n",
    "\n",
    "You can test the trained TFLite model using images from the internet. \n",
    "* Replace the `INPUT_IMAGE_URL` below with your desired input image. \n",
    "* Adjust the `DETECTION_THRESHOLD` to change the sensitivity of the model. A lower threshold means the model will pickup more objects but there will also be more false detection. Meanwhile, a higher threshold means the model will only pickup objects that it has confidently detected.\n",
    "\n",
    "Although it requires some of boilerplate code to run the model in Python at this moment, integrating the model into a mobile app only requires a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6QL_q7m3hjfA"
   },
   "outputs": [],
   "source": [
    "#@title Load the trained TFLite model and define some visualization functions\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load the labels into a list\n",
    "classes = ['???'] * model.model_spec.config.num_classes\n",
    "label_map = model.model_spec.config.label_map\n",
    "for label_id, label_name in label_map.as_dict().items():\n",
    "  classes[label_id-1] = label_name\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  return resized_img, original_image\n",
    "\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "  \"\"\"Set the input tensor.\"\"\"\n",
    "  tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  input_tensor[:, :] = image\n",
    "\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "  \"\"\"Retur the output tensor at the given index.\"\"\"\n",
    "  output_details = interpreter.get_output_details()[index]\n",
    "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "  return tensor\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "  # Feed the input image to the model\n",
    "  set_input_tensor(interpreter, image)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  boxes = get_output_tensor(interpreter, 0)\n",
    "  classes = get_output_tensor(interpreter, 1)\n",
    "  scores = get_output_tensor(interpreter, 2)\n",
    "  count = int(get_output_tensor(interpreter, 3))\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path, \n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute \n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * original_image_np.shape[1])\n",
    "    xmax = int(xmax * original_image_np.shape[1])\n",
    "    ymin = int(ymin * original_image_np.shape[0])\n",
    "    ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [int(c) for c in COLORS[class_id]]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "ixV75x-ykrQ9",
    "outputId": "ddf55693-1b9f-4d9d-ea7f-651556f1bfca"
   },
   "outputs": [],
   "source": [
    "#@title Run object detection and show the detection results\n",
    "\n",
    "model_path = 'model.tflite'\n",
    "#INPUT_IMAGE_URL = \"https://techtime.news/wp-content/uploads/sites/2/2017/10/Cognata-Simulation-Engine.png\" #@param {type:\"string\"}\n",
    "INPUT_IMAGE_URL = \"https://carla.readthedocs.io/en/0.9.7/img/low_quality_capture.png\"\n",
    "\n",
    "DETECTION_THRESHOLD = 0.3 #@param {type:\"number\"}\n",
    "\n",
    "TEMP_FILE = '/tmp/result.png'\n",
    "\n",
    "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
    "im = Image.open(TEMP_FILE)\n",
    "im.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "im.save(TEMP_FILE, 'PNG')\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    TEMP_FILE, \n",
    "    interpreter, \n",
    "    threshold=DETECTION_THRESHOLD\n",
    ")\n",
    "\n",
    "# Show the detection result\n",
    "im = Image.fromarray(detection_result_image)\n",
    "im.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "im.save(\"result2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BI3JJw8ULYGd",
    "outputId": "f1d1965a-38f6-41e6-a46e-0c29dad4e36b"
   },
   "outputs": [],
   "source": [
    "#!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "\n",
    "#!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "\n",
    "#!sudo apt-get update\n",
    "\n",
    "#!sudo apt-get install edgetpu-compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZCse8rZLWMB",
    "outputId": "1fd54af9-f8ac-4a4e-907d-164c71665cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge TPU Compiler version 15.0.340273435\n",
      "\n",
      "Model compiled successfully in 6159 ms.\n",
      "\n",
      "Input model: model.tflite\n",
      "Input size: 7.04MiB\n",
      "Output model: model_edgetpu.tflite\n",
      "Output size: 9.42MiB\n",
      "On-chip memory used for caching model parameters: 7.26MiB\n",
      "On-chip memory remaining for caching model parameters: 768.00B\n",
      "Off-chip memory used for streaming uncached model parameters: 159.12KiB\n",
      "Number of Edge TPU subgraphs: 1\n",
      "Total number of operations: 357\n",
      "Operation log: model_edgetpu.log\n",
      "\n",
      "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
      "Number of operations that will run on Edge TPU: 351\n",
      "Number of operations that will run on CPU: 6\n",
      "See the operation log file for individual operation details.\n"
     ]
    }
   ],
   "source": [
    "!edgetpu_compiler --min_runtime_version 13 model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "C-m0GWe2j4j4",
    "outputId": "4b124c1a-dd44-451e-b157-f7225ccb7168"
   },
   "outputs": [],
   "source": [
    "'''# Create labelmap from train_data\n",
    "def convert_classes(ids, label_map_dic, start=1):\n",
    "    msg = ''\n",
    "    for id_ in ids:\n",
    "        msg = msg + \"item {\\n\"\n",
    "        msg = msg + \" id: \" + str(id_) + \"\\n\"\n",
    "        msg = msg + \" name: '\" + label_map_dic[id_] + \"'\\n}\\n\\n\"\n",
    "    return msg[:-1]\n",
    "\n",
    "ids = list(train_data.label_map.keys())\n",
    "label_map_dic = train_data.label_map\n",
    "\n",
    "label_map = convert_classes(ids, label_map_dic)\n",
    "with open(\"model_label_map.pbtxt\", \"w\") as f:\n",
    "    f.write(label_map)\n",
    "    f.close()\n",
    "    print(\"Done!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnABj__RlUsc"
   },
   "outputs": [],
   "source": [
    "#print(train_data.label_map)\n",
    "#!cp /tmp/tmpu59t79wt/train_d097cf62ad50863cf2d989dd68fa080b_annotations.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCxFOY-ggthD"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/tensorflow/models.git\n",
    "# Install the Object Detection API\n",
    "\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSRQglEtAZGf"
   },
   "outputs": [],
   "source": [
    "# This is an example of using \n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_pascal_tf_record.py\n",
    "# The structure should be like PASCAL VOC format dataset\n",
    "# +Dataset\n",
    "#   +Annotations\n",
    "#   +JPEGImages\n",
    "# python create_tfrecords_from_xml.py --image_dir=dataset/JPEGImages \n",
    "#                                      --annotations_dir=dataset/Annotations \n",
    "#                                      --label_map_path=object-detection.pbtxt \n",
    "#                                      --output_path=data.record\n",
    "%tb\n",
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from lxml import etree\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "\n",
    "def dict_to_tf_example(data, image_dir, label_map_dict):\n",
    "    \"\"\"Convert XML derived dict to tf.Example proto.\n",
    "\n",
    "    Notice that this function normalizes the bounding\n",
    "    box coordinates provided by the raw data.\n",
    "\n",
    "    Arguments:\n",
    "        data: dict holding XML fields for a single image (obtained by\n",
    "          running dataset_util.recursive_parse_xml_to_dict)\n",
    "        image_dir: Path to image directory.\n",
    "        label_map_dict: A map from string label names to integers ids.\n",
    "\n",
    "    Returns:\n",
    "        example: The converted tf.Example.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(image_dir, data['filename'].replace(\"png\", \"jpeg\"))\n",
    "    with tf.compat.v1.gfile.GFile(full_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    #if image.format != 'JPEG':\n",
    "    #    raise ValueError('Image format not JPEG')\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    width = int(data['size']['width'])\n",
    "    height = int(data['size']['height'])\n",
    "\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    try:\n",
    "        for obj in data['object']:\n",
    "            xmin.append(float(obj['bndbox']['xmin']) / width)\n",
    "            ymin.append(float(obj['bndbox']['ymin']) / height)\n",
    "            xmax.append(float(obj['bndbox']['xmax']) / width)\n",
    "            ymax.append(float(obj['bndbox']['ymax']) / height)\n",
    "            classes_text.append(obj['name'].encode('utf8'))\n",
    "            classes.append(label_map_dict[obj['name']])\n",
    "    except KeyError:\n",
    "        print(data['filename'] + ' without objects!')\n",
    "\n",
    "    difficult_obj = [0]*len(classes)\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(data['filename'].replace(\"png\", \"jpeg\").encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')),\n",
    "        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/difficult': dataset_util.int64_list_feature(difficult_obj)\n",
    "    }))\n",
    "    return example\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\n",
    "    SUBSET = 'test'\n",
    "    dataset_dir = 'carla_training'\n",
    "    \n",
    "    writer = tf.compat.v1.python_io.TFRecordWriter('carla_training/test.record')\n",
    "    label_map_dict = label_map_util.get_label_map_dict(os.path.join(dataset_dir, 'carla_label_map.pbtxt'))\n",
    "    \n",
    "    image_dir = os.path.join(dataset_dir, SUBSET, 'images_dir')\n",
    "    annotations_dir = os.path.join(dataset_dir, SUBSET, 'annotations_dir')\n",
    "    logging.info('Reading from dataset: ' + annotations_dir)\n",
    "    examples_list = os.listdir(annotations_dir)\n",
    "\n",
    "    for idx, example in enumerate(examples_list):\n",
    "        if example.endswith('.xml'):\n",
    "            if idx % 50 == 0:\n",
    "                print('On image %d of %d' % (idx, len(examples_list)))\n",
    "\n",
    "            path = os.path.join(annotations_dir, example)\n",
    "            with tf.compat.v1.gfile.GFile(path, 'r') as fid:\n",
    "                xml_str = fid.read()\n",
    "            xml = etree.fromstring(xml_str)\n",
    "            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n",
    "\n",
    "            tf_example = dict_to_tf_example(data, image_dir, label_map_dict)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ubqp92CuRUBz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tno0NMqlCxQW",
    "outputId": "dfc7b159-f737-4e7a-b015-e6315dc43a1a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoEw0I8OXgIw",
    "outputId": "f1df2c83-e629-4599-a305-f4ca81a88afe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkE17-CobnSV",
    "outputId": "69e13b79-2c1a-4826-ef6b-bf32d6b116ec"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Train a salad detector with TFLite Model Maker",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
